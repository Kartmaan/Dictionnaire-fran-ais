{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analyzer\n",
    "Tool for multi-filtering words in a Pandas dataframe according to specific conditions :\n",
    "\n",
    "- No compound words\n",
    "- N-letter words\n",
    "- Words starting with\n",
    "- Words ending with\n",
    "- Words that must contain such letters\n",
    "- Words that must not contain such letters\n",
    "- Words that must contain such letters at such rank\n",
    "\n",
    "## Explanation of the arguments of the multi_filters function\n",
    "The 7 filters of the function are arranged in series.On each passage through an activated filter, the dataframe is transformed (by losing lines according to the desired criteria) until reaching the output of the function.\n",
    "\n",
    "### no_comp \n",
    "**Type** : bool\n",
    "\n",
    "**Default** : `True`\n",
    "\n",
    "Delete compound words (with spaces and dashes) in order to allow the filter to compose only with whole words\n",
    "\n",
    "`multi_filters(df, col_name =\"Mot\", no_comp=True)`\n",
    "\n",
    "### length\n",
    "**Type** : int\n",
    "\n",
    "**Default** : `None`\n",
    "\n",
    "Filter words according to a given length\n",
    "\n",
    "`multi_filters(df, col_name =\"Mot\", length=7)`\n",
    "\n",
    "### start_with\n",
    "**Type** : str\n",
    "\n",
    "**Default** : `None`\n",
    "\n",
    "Filter words starting with one or more letters given in a str\n",
    "\n",
    "`multi_filters(df, col_name =\"Mot\", start_with=\"Per\")`\n",
    "\n",
    "### end_with\n",
    "**Type** : str\n",
    "\n",
    "**Default** : `None`\n",
    "\n",
    "Filter words ending with one or more letters given in a str\n",
    "\n",
    "`multi_filters(df, col_name =\"Mot\", end_with=\"eur\")`\n",
    "\n",
    "### contains\n",
    "**Type** : list\n",
    "\n",
    "**Default** : `None`\n",
    "\n",
    "Filter words according to the letters they must contain\n",
    "\n",
    "`multi_filters(df, col_name =\"Mot\", contains=[\"r\", \"e\", \"t\"])`\n",
    "\n",
    "### not_contain\n",
    "**Type** : list\n",
    "\n",
    "**Default** : `None`\n",
    "\n",
    "Filter words according to the letters they must NOT contain\n",
    "\n",
    "`multi_filters(df, col_name =\"Mot\", not_contain=[\"v\", \"i\", \"m\"])`\n",
    "\n",
    "### nth_letters\n",
    "**Type** : list of lists\n",
    "\n",
    "**Default** : `None`\n",
    "\n",
    "Filters words according to the letters they must contain at specific ranks. Each sub-element must be a list containing `[desired rank (int), desired letter (str)]`\n",
    "\n",
    "`multi_filters(df, col_name =\"Mot\", nth_letters=[[2,\"t\"],[5,\"r\"]])`\n",
    "\n",
    "### log\n",
    "**Type** : str\n",
    "\n",
    "**Default** : \"info\"\n",
    "\n",
    "Define the level of logging to display, by default is set to \"info\" which will display messages of this level and higher. In \"debug\" mode, advanced statistics are displayed each time the filter is passed, including the execution time, the number of lines deleted, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframe of the dictionary dico.csv\n",
    "df = pd.read_csv(\"dico.csv\")\n",
    "df = df.sort_values(\"Mot\")\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "INIT_ROWS = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent(partial, total, rnd=2):\n",
    "    \"\"\" Calculates the percentage changes of rows deleted \n",
    "    on each filter pass \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return round((partial/total)*100, rnd)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(filter, start_time, end_time, rows_before, rows_after):\n",
    "    \"\"\" Displays debug messages on each filter pass when the 'log' \n",
    "    argument of the 'multi_filters' function is 'debug' \"\"\"\n",
    "    \n",
    "    exec_time = round(end_time-start_time, 3)\n",
    "    rows_del = rows_before - rows_after\n",
    "    relative_vario = percent(rows_before - rows_after, rows_before)\n",
    "    global_vario = percent(INIT_ROWS - rows_after, INIT_ROWS)\n",
    "\n",
    "    logging.debug(f\"\"\"\n",
    "        --- '{filter}' FILTER --- \n",
    "        Execution time : {exec_time}s\n",
    "        Rows before : {rows_before} \n",
    "        Rows after : {rows_after}\n",
    "        Rows deleted : {rows_before - rows_after} \n",
    "        Ponctual vario : (-{relative_vario}%)\n",
    "        Global vario : (-{global_vario}%)\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_filters(df, col_name, no_comp=True, length=None, start_with=None, \n",
    "end_with=None, nth_letters=None, contains=None, not_contain=None,\n",
    "log=\"info\"):\n",
    "\n",
    "    \"\"\" Allows multiple filters to be applied to \n",
    "    dictionary words :\n",
    "\n",
    "    :param df : Pandas dataframe of dico.csv\n",
    "    :type df : pandas.core.frame.DataFrame\n",
    "\n",
    "    :param col_name : column name to filter in dataframe\n",
    "    :type col_name : str\n",
    "\n",
    "    :param no_comp : Remove compound words\n",
    "    :type no_comp : bool\n",
    "\n",
    "    :param length : Word length\n",
    "    :type length : int\n",
    "\n",
    "    :param start_with : Letter(s) with which the word must start\n",
    "    :type start_with : str \n",
    "\n",
    "    :param end_with : Letter(s) with which the word must end\n",
    "    :type end_with : str\n",
    "\n",
    "    :param nth_letter = The letter that the word must contain at rank n.\n",
    "    :type nth_letter : list\n",
    "\n",
    "    :param contains = Letters that the word must contain\n",
    "    :type contains = list\n",
    "\n",
    "    :param not_contain = Letters that the word must NOT contain\n",
    "    :type not_contain = list\n",
    "\n",
    "    :param log = Enable logging with the desired level (debug, info, warning, critical)\n",
    "    can be set at None in this case only the CRITICAL will be displayed\n",
    "    :type log = str\n",
    "\n",
    "    :param return = Returns a new filtered dataframe\n",
    "    :type return = pandas.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    # Logging initialisation\n",
    "    if log != None:\n",
    "        log = log.upper()\n",
    "        if log == \"DEBUG\":\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        elif log == \"INFO\":\n",
    "            logger.setLevel(logging.INFO)\n",
    "        elif log == \"WARNING\":\n",
    "            logger.setLevel(logging.WARNING)\n",
    "        elif log == \"CRITICAL\":\n",
    "            logger.setLevel(logging.CRITICAL)\n",
    "        else:\n",
    "            logger.setLevel(logging.CRITICAL)\n",
    "    else:\n",
    "        logger.setLevel(logging.CRITICAL)\n",
    "    \n",
    "    # ------------ DATAFRAME CHECKS ------------\n",
    "    # Dataframe check\n",
    "    if type(df) != pd.core.frame.DataFrame:\n",
    "        logging.critical(f\"\"\"\n",
    "        df must be a Pandas dataframe. {type(df)} given \"\"\")\n",
    "        return None\n",
    "        \n",
    "    elif col_name not in df.columns:\n",
    "        logging.critical(f\"\"\"\n",
    "        '{col_name}' column doesn't exist in the dataframe.\n",
    "        Columns present : {[col for col in df.columns]}\"\"\")\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # ------------ CONFLICTS CHECK ------------\n",
    "    # contains/not_contain check\n",
    "    if contains != None and not_contain != None:\n",
    "        if not isinstance(contains, list) or not isinstance(not_contain, list):\n",
    "            logging.critical(f\"\"\"'contains' or 'not_contain' isn't a list\"\"\")\n",
    "            return None\n",
    "\n",
    "        elif set(contains) & set(not_contain):\n",
    "            logging.critical(f\"\"\"\n",
    "            'contains' and 'not_contain' must not share common values  \"\"\")\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    INIT_TIME = time()\n",
    "    INIT_SHAPE = df.shape[0]\n",
    "    filters_crossed = []\n",
    "\n",
    "    logging.debug(f\"\"\"\n",
    "    -- INITIAL VALUES --\n",
    "    Start at : {INIT_TIME}\n",
    "    Dataframe shape : {df.shape}\n",
    "    Column to filter : {col_name}\n",
    "    no_comp = {no_comp}\n",
    "    length = {length}\n",
    "    start_with = {start_with}\n",
    "    end_with = {end_with}\n",
    "    nth_letters = {nth_letters}\n",
    "    contains = {contains}\n",
    "    not_contain = {not_contain}\n",
    "    \"\"\")\n",
    "\n",
    "    # ------------ FILTERS ------------\n",
    "    # FILTER 1/ No compound words\n",
    "    if no_comp:\n",
    "        start_time = time()\n",
    "\n",
    "        df = df.loc[\n",
    "        (~df[col_name].str.contains(r'\\s')) & \n",
    "        (~df[col_name].str.contains(r'-'))\n",
    "        ]\n",
    "\n",
    "        end_time = time()\n",
    "        debug(\"no_comp\", start_time, end_time, INIT_SHAPE, df.shape[0])\n",
    "        filters_crossed.append(\"no_comp\")\n",
    "    \n",
    "    # FILTER 2/ By word length\n",
    "    if length != None:\n",
    "        if not isinstance(length, int):\n",
    "            logging.critical(f\"\"\"'length' must be of type int. \n",
    "            {type(length)} given\"\"\")\n",
    "            return None\n",
    "\n",
    "        ponctual_shape = df.shape[0]\n",
    "        start_time = time()\n",
    "\n",
    "        df = df.loc[df[col_name].str.len() == length]\n",
    "\n",
    "        end_time = time()\n",
    "        debug(\"length\", start_time, end_time, ponctual_shape, df.shape[0])\n",
    "        filters_crossed.append(\"length\")\n",
    "    \n",
    "    # FILTER 3/ By absence of letters\n",
    "    if not_contain != None:\n",
    "        if not isinstance(not_contain, list):\n",
    "            logging.critical(f\"\"\"'not_contain' must be of type list. \n",
    "            {type(not_contain)} given\"\"\")\n",
    "            return None\n",
    "\n",
    "        elif not all(type(x) == str for x in not_contain):\n",
    "            logging.critical(\"\"\"One of the elements of 'not_contain' \n",
    "            is not a str.\"\"\")\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        not_contain = set(not_contain) # remove duplicates\n",
    "        r = \"\"\n",
    "        for lettre in not_contain:\n",
    "            r = r + f\"(?=.*{lettre})\"\n",
    "\n",
    "        regex = f\"^{r}.*$\"\n",
    "\n",
    "        ponctual_shape = df.shape[0]\n",
    "        start_time = time()\n",
    "\n",
    "        df = df.loc[~df[col_name].str.contains(regex)] # ~ for negation\n",
    "\n",
    "        end_time = time()\n",
    "        debug(\"not_contain\", start_time, end_time, ponctual_shape, df.shape[0])\n",
    "        filters_crossed.append(\"not_contain\")\n",
    "    \n",
    "    # FILTER 4/ By presence of letters\n",
    "    if contains != None:\n",
    "        if not isinstance(contains, list):\n",
    "            logging.critical(f\"\"\"'contains' must be of type list. \n",
    "            {type(contains)} given\"\"\")\n",
    "            return None\n",
    "\n",
    "        elif not all(type(x) == str for x in contains):\n",
    "            logging.critical(\"\"\"One of the elements of 'contains' \n",
    "            is not a str.\"\"\")\n",
    "            return None\n",
    "\n",
    "        contains = set(contains) # remove duplicates\n",
    "        r = \"\"\n",
    "        for lettre in contains:\n",
    "            r = r + f\"(?=.*{lettre})\"\n",
    "\n",
    "        regex = f\"^{r}.*$\"\n",
    "\n",
    "        ponctual_shape = df.shape[0]\n",
    "        start_time = time()\n",
    "\n",
    "        df = df.loc[df[col_name].str.contains(regex)]\n",
    "\n",
    "        end_time = time()\n",
    "        debug(\"contains\", start_time, end_time, ponctual_shape, df.shape[0])\n",
    "        filters_crossed.append(\"contains\")\n",
    "    \n",
    "    # FILTER 5/ By beginning of word\n",
    "    if start_with != None:\n",
    "        if not isinstance(start_with, str):\n",
    "            logging.critical(f\"\"\"'start_with' must be of type str. \n",
    "            {type(start_with)} given\"\"\")\n",
    "            return None\n",
    "\n",
    "        start_with = start_with.capitalize()\n",
    "\n",
    "        ponctual_shape = df.shape[0]\n",
    "        start_time = time()\n",
    "\n",
    "        df = df.loc[df[col_name].str.startswith(start_with)]\n",
    "\n",
    "        end_time = time()\n",
    "        debug(\"start_with\", start_time, end_time, ponctual_shape, df.shape[0])\n",
    "        filters_crossed.append(\"start_with\")\n",
    "    \n",
    "    # FILTER 6/ By letters position\n",
    "    if nth_letters != None:\n",
    "        if not isinstance(nth_letters, list):\n",
    "            logging.critical(f\"\"\"'nth_letters' must be of type list. \n",
    "            {type(nth_letters)} given\"\"\")\n",
    "            return None\n",
    "        \n",
    "        elif not all(type(x)==list and len(x)==2 for x in nth_letters):\n",
    "            logging.critical(f\"\"\"All elements of the 'nth letters' list \n",
    "            must be lists of 2 elements: [rank, letter]\"\"\")\n",
    "            return None\n",
    "        \n",
    "        elif not all(type(x[0])==int and type(x[1])==str \n",
    "        and len(x[1])==1 for x in nth_letters):\n",
    "            logging.critical(f\"\"\"Each sub-element of nth_letters must be a list \n",
    "            composed of 2 elements [rank(int), 1 letter (str)]\"\"\")\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        nth_letters = dict(nth_letters)\n",
    "        \n",
    "        ponctual_shape = df.shape[0]\n",
    "        start_time = time()\n",
    "\n",
    "        for rank, letter in nth_letters.items():\n",
    "            df = df.loc[df[col_name].apply(lambda x: len(x) > rank and x[rank-1] == letter)]\n",
    "\n",
    "        end_time = time()\n",
    "        debug(\"nth_letters\", start_time, end_time, ponctual_shape, df.shape[0])\n",
    "        filters_crossed.append(\"nth_letters\")\n",
    "\n",
    "    # FILTER 7/ By ending of word\n",
    "    if end_with != None:\n",
    "        if not isinstance(start_with, str):\n",
    "            logging.critical(f\"\"\"'start_with' must be of type str. \n",
    "            {type(start_with)} given\"\"\")\n",
    "            return None\n",
    "\n",
    "        ponctual_shape = df.shape[0]\n",
    "        start_time = time()\n",
    "\n",
    "        df = df.loc[df[col_name].str.endswith(end_with)]\n",
    "\n",
    "        end_time = time()\n",
    "        debug(\"end_with\", start_time, end_time, ponctual_shape, df.shape[0])\n",
    "        filters_crossed.append(\"end_with\")\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        logging.info(\"No words found\")\n",
    "    \n",
    "    logging.debug(f\"\"\"\n",
    "    -- FINAL STATS -- \n",
    "    Total execution time : {round(time() - INIT_TIME, 3)}s\n",
    "    Filters crossed = {len(filters_crossed)}/7 -> {filters_crossed}\n",
    "    Total rows deleted : {INIT_SHAPE - df.shape[0]}\n",
    "    From {INIT_SHAPE} to {df.shape[0]} -> (-{percent(INIT_SHAPE - df.shape[0], INIT_SHAPE, rnd=4)}%)\n",
    "    \"\"\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mot</th>\n",
       "      <th>Définitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>354317</th>\n",
       "      <td>Gantait</td>\n",
       "      <td>['Du verbe ganter.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355842</th>\n",
       "      <td>Gattait</td>\n",
       "      <td>[\"Troisième personne du singulier de l'indicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358324</th>\n",
       "      <td>Gertait</td>\n",
       "      <td>[\"Troisième personne du singulier de l'indicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365395</th>\n",
       "      <td>Goutait</td>\n",
       "      <td>[\"Troisième personne du singulier de l'indicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365794</th>\n",
       "      <td>Goûtait</td>\n",
       "      <td>['Du verbe goûter.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368175</th>\n",
       "      <td>Gratuit</td>\n",
       "      <td>[\"Qu'on donne, sans y être tenu.\", '…']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371798</th>\n",
       "      <td>Grutait</td>\n",
       "      <td>[\"Troisième personne du singulier de l'indicat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mot                                        Définitions\n",
       "354317  Gantait                               ['Du verbe ganter.']\n",
       "355842  Gattait  [\"Troisième personne du singulier de l'indicat...\n",
       "358324  Gertait  [\"Troisième personne du singulier de l'indicat...\n",
       "365395  Goutait  [\"Troisième personne du singulier de l'indicat...\n",
       "365794  Goûtait                               ['Du verbe goûter.']\n",
       "368175  Gratuit            [\"Qu'on donne, sans y être tenu.\", '…']\n",
       "371798  Grutait  [\"Troisième personne du singulier de l'indicat..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_filters(df,\n",
    "col_name=\"Mot\",\n",
    "start_with=\"g\",\n",
    "end_with=\"it\",\n",
    "contains=[\"a\"],\n",
    "not_contain=[\"b\"],\n",
    "nth_letters=[[4,\"t\"]],\n",
    "length=7\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
